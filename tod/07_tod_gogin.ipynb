{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Считайте слова из файла `litw-win.txt` и запишите их в список `words`. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка `words`. Считайте, что в слове есть опечатка, если данное слово не содержится в списке `words`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-Levenshtein in c:\\users\\shubs\\anaconda3\\lib\\site-packages (0.25.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: Levenshtein==0.25.1 in c:\\users\\shubs\\anaconda3\\lib\\site-packages (from python-Levenshtein) (0.25.1)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in c:\\users\\shubs\\anaconda3\\lib\\site-packages (from Levenshtein==0.25.1->python-Levenshtein) (3.9.3)\n"
     ]
    }
   ],
   "source": [
    "pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''с велИчайшим усилеем выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "с величайшим усилием выбравшись из потока убегающих людей кутузов со свитой уменьшившейся вдвое поехал на звуки выстрелов русских орудий\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# cчитываем слова из файла\n",
    "with open(\"litw-win.txt\", \"r\") as f:\n",
    "    words = [line.split()[1] for line in f]\n",
    "\n",
    "    \n",
    "# исправление опечаток\n",
    "def correct_word(word, words):\n",
    "    if word in words:  # если слово в words, значит исправлять не надо, просто возвращаем слово\n",
    "        return word\n",
    "    else:\n",
    "        # находим ближайшее слово по расстоянию Левенштейна\n",
    "        min_distance = float('inf')\n",
    "        corrected_word = None\n",
    "        for w in words:\n",
    "            distance = Levenshtein.distance(word, w)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                corrected_word = w\n",
    "        return corrected_word\n",
    "\n",
    "corrected_sentence = \" \".join([correct_word(word, words) for word in text.split()])\n",
    "\n",
    "print(corrected_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Разбейте текст из формулировки задания 1 на слова; проведите стемминг и лемматизацию слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригинальные слова:\n",
      "['Считайте', 'слова', 'из', 'файла', 'litw-win.txt', 'и', 'запишите', 'их', 'в', 'список', 'words', '.', 'В', 'заданном', 'предложении', 'исправьте', 'все', 'опечатки', ',', 'заменив', 'слова', 'с', 'опечатками', 'на', 'ближайшие', '(', 'в', 'смысле', 'расстояния', 'Левенштейна', ')', 'к', 'ним', 'слова', 'из', 'списка', 'words', '.', 'Считайте', ',', 'что', 'в', 'слове', 'есть', 'опечатка', ',', 'если', 'данное', 'слово', 'не', 'содержится', 'в', 'списке', 'words', '.']\n",
      "\n",
      "Лемматизированные слова:\n",
      "['считать', 'слово', 'из', 'файл', 'litw-win.txt', 'и', 'записать', 'они', 'в', 'список', 'words', '.', 'в', 'задать', 'предложение', 'исправить', 'всё', 'опечатка', ',', 'заменить', 'слово', 'с', 'опечатка', 'на', 'близкий', '(', 'в', 'смысл', 'расстояние', 'левенштейн', ')', 'к', 'они', 'слово', 'из', 'список', 'words', '.', 'считать', ',', 'что', 'в', 'слово', 'есть', 'опечатка', ',', 'если', 'данный', 'слово', 'не', 'содержаться', 'в', 'список', 'words', '.']\n",
      "\n",
      "Стеммированные слова:\n",
      "['счита', 'слов', 'из', 'файл', 'litw-win.txt', 'и', 'запиш', 'их', 'в', 'список', 'words', '.', 'в', 'зада', 'предложен', 'исправьт', 'все', 'опечатк', ',', 'замен', 'слов', 'с', 'опечатк', 'на', 'ближайш', '(', 'в', 'смысл', 'расстоян', 'левенштейн', ')', 'к', 'ним', 'слов', 'из', 'списк', 'words', '.', 'счита', ',', 'что', 'в', 'слов', 'ест', 'опечатк', ',', 'есл', 'дан', 'слов', 'не', 'содерж', 'в', 'списк', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "import pymorphy3\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "# Разбиваем текст на слова\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Создаем экземпляры MorphAnalyzer и SnowballStemmer\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "# Функции для стемминга и лемматизации\n",
    "def lemmatize_word(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "# Применяем лемматизацию и стемминг к каждому слову\n",
    "lemmatized_words = [lemmatize_word(word) for word in words]\n",
    "stemmed_words = [stem_word(word) for word in words]\n",
    "\n",
    "# Вывод результатов\n",
    "print(\"Оригинальные слова:\")\n",
    "print(words)\n",
    "print(\"\\nЛемматизированные слова:\")\n",
    "print(lemmatized_words)\n",
    "print(\"\\nСтеммированные слова:\")\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Преобразуйте предложения из формулировки задания 1 в векторы при помощи `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предложение 1: Считайте слова из файла litw-win.txt и запишите их в список words.\n",
      "Вектор:\n",
      "[[1 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0]]\n",
      "\n",
      "Предложение 2: В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words.\n",
      "Вектор:\n",
      "[[0 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 2 0 0 1 0 1 0 0 0 0 0]]\n",
      "\n",
      "Предложение 3: Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.\n",
      "Вектор:\n",
      "[[0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Предложения из задания 1\n",
    "sentences = [\n",
    "    \"Считайте слова из файла litw-win.txt и запишите их в список words.\",\n",
    "    \"В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words.\",\n",
    "    \"Считайте, что в слове есть опечатка, если данное слово не содержится в списке words.\"\n",
    "]\n",
    "\n",
    "# Инициализируем CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Преобразуем предложения в векторы\n",
    "sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# Получаем список уникальных слов в предложениях (функция get_feature_names)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Выводим предложения и соответствующие векторы\n",
    "for i, sentence in enumerate(sentences):\n",
    "    print(f\"Предложение {i + 1}: {sentence}\")\n",
    "    print(\"Вектор:\")\n",
    "    print(sentence_vectors[i].toarray())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bahamian',\n",
       " 'struck',\n",
       " 'cordial',\n",
       " 'available',\n",
       " 'streamline',\n",
       " 'coffees',\n",
       " 'mozzerella',\n",
       " 'didnt',\n",
       " 'vibrancy',\n",
       " 'fries',\n",
       " 'revecca',\n",
       " 'franskbrod',\n",
       " 'omission',\n",
       " 'mario',\n",
       " 'squishiness',\n",
       " 'heavenly',\n",
       " 'qualities',\n",
       " 'jay',\n",
       " 'chinn',\n",
       " 'bennigan',\n",
       " 'bell',\n",
       " 'waterman',\n",
       " 'dulce',\n",
       " 'reserved',\n",
       " 'rubel',\n",
       " 'flora',\n",
       " 'savoury',\n",
       " 'caribbeanchoice',\n",
       " 'dressed',\n",
       " 'roquefort',\n",
       " 'slits',\n",
       " 'kuzbara',\n",
       " 'mountain',\n",
       " 'quarterbacks',\n",
       " 'broths',\n",
       " 'peas',\n",
       " 'perkins',\n",
       " 'caloric',\n",
       " 'acompaniment',\n",
       " 'singapore',\n",
       " 'bitburg',\n",
       " 'sees',\n",
       " 'freepasta',\n",
       " 'powering',\n",
       " 'tower',\n",
       " 'mainland',\n",
       " 'deceiving',\n",
       " 'husks',\n",
       " 'earthly',\n",
       " 'mille',\n",
       " 'clothing',\n",
       " 'bologna',\n",
       " 'squeeze',\n",
       " 'susiequsie',\n",
       " 'frostings',\n",
       " 'undervalued',\n",
       " 'lure',\n",
       " 'disease',\n",
       " 'fantabulous',\n",
       " 'optimum',\n",
       " 'contast',\n",
       " 'raison',\n",
       " 'hodgon',\n",
       " 'liver',\n",
       " 'carafe',\n",
       " 'karma',\n",
       " 'overpowrer',\n",
       " 'scrap',\n",
       " 'thinned',\n",
       " 'brunches',\n",
       " 'informed',\n",
       " '96th',\n",
       " 'demonstrating',\n",
       " '56',\n",
       " 'caserra',\n",
       " 'snugly',\n",
       " 'tossing',\n",
       " 'herbal',\n",
       " 'kimble',\n",
       " 'alarmed',\n",
       " 'coffe',\n",
       " 'celsoup',\n",
       " 'swahali',\n",
       " 'pius',\n",
       " 'unmodestly',\n",
       " 'bubbleteasupply',\n",
       " 'mollasses',\n",
       " 'grad',\n",
       " 'variating',\n",
       " '59',\n",
       " 'bouncing',\n",
       " 'albi',\n",
       " 'ag',\n",
       " 'bridgeton',\n",
       " 'approaching',\n",
       " 'marques',\n",
       " 'bbqed',\n",
       " 'rehydration',\n",
       " 'candles',\n",
       " 'fastest',\n",
       " 'cecelia',\n",
       " 'brand',\n",
       " 'kahr',\n",
       " 'progressed',\n",
       " 'drawl',\n",
       " 'revisions',\n",
       " 'bldg',\n",
       " 'eveyone',\n",
       " 'hillshire',\n",
       " 'assembly',\n",
       " 'prebake',\n",
       " 'edina',\n",
       " 'bartlett',\n",
       " 'experiement',\n",
       " 'ooos',\n",
       " 'savoured',\n",
       " 'cabot',\n",
       " 'stroke',\n",
       " '185',\n",
       " 'lost',\n",
       " 'brew',\n",
       " 'unhealthy',\n",
       " 'benefits',\n",
       " 'hindi',\n",
       " 'ofr',\n",
       " 'adaptation',\n",
       " 'murcia',\n",
       " 'lavender',\n",
       " 'portion',\n",
       " 'frowns',\n",
       " 'bars',\n",
       " 'becoming',\n",
       " 'fould',\n",
       " 'folk',\n",
       " 'center',\n",
       " 'noted',\n",
       " 'arrived',\n",
       " 'company',\n",
       " 'ritas',\n",
       " 'dgustibus',\n",
       " 'neutral',\n",
       " 'envy',\n",
       " 'clifford',\n",
       " 'helman',\n",
       " 'racheal',\n",
       " 'zwarte',\n",
       " 'michy',\n",
       " 'azul',\n",
       " 'definite',\n",
       " 'shave',\n",
       " 'npr',\n",
       " 'battered',\n",
       " 'sfi',\n",
       " 'marsh',\n",
       " 'bit',\n",
       " 'pizelle',\n",
       " 'cones',\n",
       " 'uc',\n",
       " 'sheila',\n",
       " 'nile',\n",
       " 'cultural',\n",
       " 'sophistication',\n",
       " 'maurer',\n",
       " 'unpeeled',\n",
       " 'mornistar',\n",
       " 'ardmore',\n",
       " 'kvass',\n",
       " 'concepts',\n",
       " 'shoney',\n",
       " 'risk',\n",
       " 'aging',\n",
       " 'pepermint',\n",
       " 'handyman',\n",
       " 'enjoyment',\n",
       " 'roar',\n",
       " 'supper',\n",
       " 'puebla',\n",
       " 'lately',\n",
       " 'jollof',\n",
       " 'dijon',\n",
       " 'resty',\n",
       " 'tacky',\n",
       " 'disposable',\n",
       " 'stress',\n",
       " 'reminicescent',\n",
       " 'cakes',\n",
       " 'padma',\n",
       " 'gwen',\n",
       " 'spoonful',\n",
       " 'obtaining',\n",
       " 'cambozola',\n",
       " 'middle',\n",
       " 'hell',\n",
       " 'procrastinating',\n",
       " 'difficulties',\n",
       " 'tapioca',\n",
       " 'elevate',\n",
       " 'samih',\n",
       " 'leaner',\n",
       " 'englander',\n",
       " 'kn3kj7he',\n",
       " 'eater',\n",
       " 'literaly',\n",
       " 'metals',\n",
       " 'hunger',\n",
       " 'qeth',\n",
       " 'suits',\n",
       " 'suffice',\n",
       " 'donohue',\n",
       " 'muenster',\n",
       " 'honoring',\n",
       " 'races',\n",
       " 'identifiable',\n",
       " 'dazzling',\n",
       " 'scapes',\n",
       " 'legion',\n",
       " 'chef1mom',\n",
       " 'croquetas',\n",
       " 'work',\n",
       " 'blessum',\n",
       " 'blame',\n",
       " 'stones',\n",
       " 'peal',\n",
       " 'alway',\n",
       " 'slightly',\n",
       " 'foaming',\n",
       " 'nausea',\n",
       " 'patisserie',\n",
       " 'confectionary',\n",
       " 'customs',\n",
       " 'odities',\n",
       " 'kabbage',\n",
       " 'epicure',\n",
       " 'covent',\n",
       " 'calm',\n",
       " 'wasteful',\n",
       " 'posh',\n",
       " 'wally',\n",
       " 'thermometor',\n",
       " 'hyderabad',\n",
       " 'idea',\n",
       " 'endangered',\n",
       " 'hack',\n",
       " 'laughton',\n",
       " 'conversions',\n",
       " 'deepfried',\n",
       " 'transparent',\n",
       " 'villa',\n",
       " '189040',\n",
       " 'reuben',\n",
       " 'turnovers',\n",
       " 'seamlessly',\n",
       " 'membrane',\n",
       " 'evreryone',\n",
       " 'anw0792',\n",
       " 'cloudy',\n",
       " 'necks',\n",
       " 'marks',\n",
       " 'wicked',\n",
       " 'approx',\n",
       " 'lassi',\n",
       " 'server',\n",
       " 'temperature',\n",
       " 'accomodate',\n",
       " 'adored',\n",
       " 'sippin',\n",
       " 'dealer',\n",
       " 'turned',\n",
       " 'drop',\n",
       " 'crabs',\n",
       " 'emits',\n",
       " 'compatible',\n",
       " 'romania',\n",
       " 'soup',\n",
       " 'zagreb',\n",
       " 'ox',\n",
       " 'aired',\n",
       " 'malouf',\n",
       " 'cantalope',\n",
       " 'unmilled',\n",
       " '53',\n",
       " 'concensus',\n",
       " 'nabo',\n",
       " 'packing',\n",
       " 'immediately',\n",
       " 'coyote',\n",
       " 'flung',\n",
       " 'warcraft',\n",
       " 'decorating',\n",
       " 'tortillas',\n",
       " 'pumpkin',\n",
       " 'monterey',\n",
       " 'harissa',\n",
       " 'porto',\n",
       " 'oscar',\n",
       " 'chronic',\n",
       " 'celtic',\n",
       " 'arum',\n",
       " 'bruch',\n",
       " 'missmew2',\n",
       " 'morningstar',\n",
       " 'differently',\n",
       " 'engine',\n",
       " 'boysenberries',\n",
       " 'complimenting',\n",
       " 'merge',\n",
       " 'draught',\n",
       " 'ras',\n",
       " 'chipotle',\n",
       " 'kochenmachtspass',\n",
       " 'just',\n",
       " 'shadows',\n",
       " 'paradisi',\n",
       " 'chemist',\n",
       " 'chantilly',\n",
       " 'nada',\n",
       " 'chao',\n",
       " 'coffeecakes',\n",
       " 'gigondas',\n",
       " 'reduced',\n",
       " 'hunkered',\n",
       " 'kulich',\n",
       " 'lanni',\n",
       " 'convienent',\n",
       " 'touches',\n",
       " 'baguettes',\n",
       " 'makinglifebetter',\n",
       " 'addressed',\n",
       " 'advertising',\n",
       " 'wrigley',\n",
       " 'gregg',\n",
       " 'bottega',\n",
       " 'sherman',\n",
       " 'mccargo',\n",
       " 'slurping',\n",
       " 'immigrated',\n",
       " 'reed',\n",
       " '194921',\n",
       " 'delia',\n",
       " 'barrio',\n",
       " 'burrell',\n",
       " 'rough',\n",
       " 'getting',\n",
       " 'merrett',\n",
       " 'yosenabe',\n",
       " 'lotus',\n",
       " 'guillermo',\n",
       " 'uo',\n",
       " 'drool',\n",
       " 'games',\n",
       " 'wyk',\n",
       " 'phrase',\n",
       " 'poppy',\n",
       " 'sallybernstein',\n",
       " 'hastings',\n",
       " 'chews',\n",
       " 'okras',\n",
       " 'subtropical',\n",
       " 'tones',\n",
       " 'key',\n",
       " 'fo',\n",
       " 'compliments',\n",
       " 'empty',\n",
       " 'diabretic',\n",
       " 'kneaded',\n",
       " 'sabzi',\n",
       " 'skyline',\n",
       " 'harvard',\n",
       " 'moonlight',\n",
       " 'blesses',\n",
       " 'abs',\n",
       " 'checking',\n",
       " 'rooibos',\n",
       " 'maricopa',\n",
       " 'wildernessfamilynaturals',\n",
       " '135453',\n",
       " 'bluff',\n",
       " 'topic',\n",
       " 'wix',\n",
       " 'myrtle',\n",
       " 'shooters',\n",
       " 'grub',\n",
       " 'freezable',\n",
       " 'cucina',\n",
       " 'conscious',\n",
       " '26954',\n",
       " 'combinaton',\n",
       " 'puck',\n",
       " 'ofcourse',\n",
       " 'syria',\n",
       " 'bechemel',\n",
       " 'cicle',\n",
       " 'falafel',\n",
       " 'creamiest',\n",
       " 'didn',\n",
       " 'unlikely',\n",
       " 'excitement',\n",
       " 'victorian',\n",
       " 'curative',\n",
       " 'erindipity',\n",
       " 'you',\n",
       " 'lining',\n",
       " 'stanton',\n",
       " 'plethora',\n",
       " 'souuth',\n",
       " 'uncommonly',\n",
       " 'unsatisfactory',\n",
       " '1895',\n",
       " 'zwt6',\n",
       " 'mersaut',\n",
       " 'wolfert',\n",
       " 'seeings',\n",
       " 'okara',\n",
       " 'pawpaw',\n",
       " 'fiancee',\n",
       " 'regionally',\n",
       " 'candies',\n",
       " 'mos',\n",
       " 'gourment',\n",
       " 'longa',\n",
       " '15yo',\n",
       " 'dollar',\n",
       " 'boca',\n",
       " 'opo',\n",
       " 'produced',\n",
       " 'busiest',\n",
       " 'referring',\n",
       " 'petersburg',\n",
       " 'moruga',\n",
       " 'danny',\n",
       " 'descriptions',\n",
       " 'vincenzo',\n",
       " 'puff',\n",
       " 'forbid',\n",
       " 'lightning',\n",
       " 'catalina',\n",
       " 'approximately',\n",
       " '184513',\n",
       " 'magarian',\n",
       " '2lbs',\n",
       " 'beet',\n",
       " '177110',\n",
       " 'endearment',\n",
       " 'lit',\n",
       " 'bottle',\n",
       " 'dawa',\n",
       " 'ant',\n",
       " 'kissing',\n",
       " 'emulsifies',\n",
       " 'folks',\n",
       " 'sparkle',\n",
       " 'bacterial',\n",
       " 'regarding',\n",
       " 'rubios',\n",
       " 'door',\n",
       " 'smokey',\n",
       " 'client',\n",
       " 'spiciness',\n",
       " 'vh1',\n",
       " '458602',\n",
       " 'dumpling',\n",
       " 'comic',\n",
       " 'mission',\n",
       " 'monthly',\n",
       " 'gotten',\n",
       " 'pasteurized',\n",
       " 'responsibility',\n",
       " 'dekmak',\n",
       " 'cajunsausage',\n",
       " 'fathom',\n",
       " 'gma',\n",
       " 'grade',\n",
       " 'vegie',\n",
       " 'bay',\n",
       " 'avons',\n",
       " 'sub',\n",
       " 'katy',\n",
       " 'centered',\n",
       " 'ellison',\n",
       " 'tantalizingly',\n",
       " 'nuts',\n",
       " 'dukka',\n",
       " 'carnivore',\n",
       " 'screen',\n",
       " 'walleye',\n",
       " 'chilly',\n",
       " 'entitled',\n",
       " 'wander',\n",
       " 'basin',\n",
       " 'phones',\n",
       " 'biking',\n",
       " 'values',\n",
       " 'vegcooking',\n",
       " 'dormroom',\n",
       " 'harried',\n",
       " 'beprepared',\n",
       " 'dietitians',\n",
       " 'girlfriend',\n",
       " 'pin',\n",
       " 'cherished',\n",
       " 'asafoetida',\n",
       " 'theveggiegal',\n",
       " 'zwt5',\n",
       " 'debby',\n",
       " 'nutrious',\n",
       " 'zakuski',\n",
       " 'awayyyyyyyy',\n",
       " 'drinky',\n",
       " 'satisfactory',\n",
       " 'eels',\n",
       " 'guaranteed',\n",
       " 'dismissed',\n",
       " 'teething',\n",
       " 'ancestry',\n",
       " 'fliping',\n",
       " 'testing',\n",
       " '45th',\n",
       " 'adoption',\n",
       " '3abn',\n",
       " '04',\n",
       " 'durring',\n",
       " 'oakland',\n",
       " 'adapted',\n",
       " 'reportedly',\n",
       " 'gold',\n",
       " 'netherlands',\n",
       " 'whitley',\n",
       " 'elaboration',\n",
       " 'berber',\n",
       " 'kreplach',\n",
       " 'activate',\n",
       " 'ricardo',\n",
       " 'scrambling',\n",
       " 'chocolat',\n",
       " 'tor',\n",
       " 'daas',\n",
       " 'mediterrran',\n",
       " 'transliterated',\n",
       " 'rumbledethumps',\n",
       " 'gig',\n",
       " 'crabcakes',\n",
       " 'few',\n",
       " 'wenatchee',\n",
       " 'branch',\n",
       " 'parent',\n",
       " 'introducing',\n",
       " 'friendly',\n",
       " 'cheryl',\n",
       " 'cavaiani',\n",
       " 'wonton',\n",
       " 'vengeance',\n",
       " 'eing',\n",
       " 'manchego',\n",
       " 'nope',\n",
       " 'sigh',\n",
       " 'chicago',\n",
       " 'parenthesis',\n",
       " 'fructose',\n",
       " 'treliving',\n",
       " 'boiling',\n",
       " 'klivans',\n",
       " 'verts',\n",
       " 'wiz',\n",
       " 'meatiness',\n",
       " 'dear',\n",
       " 'that',\n",
       " 'caterpillar',\n",
       " 'impressing',\n",
       " 'loser',\n",
       " 'granger',\n",
       " 'apologies',\n",
       " 'whipping',\n",
       " 'dissapointed',\n",
       " '50',\n",
       " 'baresi',\n",
       " '1930',\n",
       " 'shaw',\n",
       " 'muy',\n",
       " 'inspired',\n",
       " 'tableside',\n",
       " 'recognition',\n",
       " 'irresistable',\n",
       " 'underwood',\n",
       " 'quickie',\n",
       " 'messes',\n",
       " 'botulism',\n",
       " 'lcbo',\n",
       " 'chateau',\n",
       " 'chinatown',\n",
       " 'magical',\n",
       " 'hubs',\n",
       " 'tell',\n",
       " 'sauerkraut',\n",
       " 'gate',\n",
       " 'medley',\n",
       " 'origioually',\n",
       " '167663',\n",
       " 'chores',\n",
       " 'noticeably',\n",
       " 'unevenly',\n",
       " 'knowing',\n",
       " 'yucatan',\n",
       " 'drawer',\n",
       " 'stopping',\n",
       " 'schlotsky',\n",
       " 'drama',\n",
       " 'sparkrecipes',\n",
       " 'whites',\n",
       " 'greasing',\n",
       " 'communityprograms',\n",
       " 'johnston',\n",
       " 'oklahoma',\n",
       " 'fleishmanns',\n",
       " 'cusine',\n",
       " 'fideo',\n",
       " '64',\n",
       " 'baste',\n",
       " 'wedges',\n",
       " 'tailoring',\n",
       " '425083',\n",
       " 'careful',\n",
       " 'recipes4us',\n",
       " 'buffet',\n",
       " 'pineaple',\n",
       " 'breakdown',\n",
       " 'experienced',\n",
       " '1942',\n",
       " 'magnesium',\n",
       " '312721',\n",
       " 'tore',\n",
       " 'farmgirl',\n",
       " 'email',\n",
       " 'yesterday',\n",
       " '5points',\n",
       " 'trentino',\n",
       " 'parmesean',\n",
       " 'challahs',\n",
       " 'ball',\n",
       " 'goals',\n",
       " 'thesinglebite',\n",
       " 'chan',\n",
       " 'ive',\n",
       " 'warmers',\n",
       " 'loux',\n",
       " 'punjabis',\n",
       " 'diluting',\n",
       " 'pomi',\n",
       " 'proven',\n",
       " 'hassle',\n",
       " 'ea',\n",
       " 'banff',\n",
       " '1929',\n",
       " 'wied',\n",
       " 'granddaughter',\n",
       " 'bock',\n",
       " 'moms',\n",
       " '10088',\n",
       " 'vaugtin',\n",
       " 'sorbet',\n",
       " 'buck',\n",
       " 'business',\n",
       " 'ceiling',\n",
       " 'counselor',\n",
       " 'qualifies',\n",
       " 'gables',\n",
       " 'roma',\n",
       " 'lookig',\n",
       " 'ulla',\n",
       " 'firstpublished',\n",
       " 'x9',\n",
       " 'wheeler',\n",
       " 'msn',\n",
       " 'veggie',\n",
       " 'bunt',\n",
       " 'kokopelli',\n",
       " 'quart',\n",
       " 'teeth',\n",
       " 'bushi',\n",
       " 'besure',\n",
       " 'group',\n",
       " 'midnight',\n",
       " 'confirmed',\n",
       " 'contrast',\n",
       " 'aka',\n",
       " 'anther',\n",
       " 'kill',\n",
       " 'tibetan',\n",
       " 'rz',\n",
       " 'chez',\n",
       " 'gatlinburg',\n",
       " '256',\n",
       " 'nahm',\n",
       " 'rojo',\n",
       " 'oudoor',\n",
       " '6pts',\n",
       " 'momokawa',\n",
       " 'guiliano',\n",
       " 'picnics',\n",
       " 'shacks',\n",
       " 'ayam',\n",
       " 'ajo',\n",
       " 'chameleon',\n",
       " 'scrubbed',\n",
       " 'toast',\n",
       " 'appealing',\n",
       " 'un',\n",
       " 'grocers',\n",
       " 'fry',\n",
       " 'mendelson',\n",
       " 'spas',\n",
       " 'prompted',\n",
       " 'dba',\n",
       " 'growing',\n",
       " 'heering',\n",
       " 'ipie',\n",
       " 'poissons',\n",
       " 'v8',\n",
       " 'teen',\n",
       " 'moistest',\n",
       " 'misread',\n",
       " 'bodes',\n",
       " 'spanich',\n",
       " 'handbook',\n",
       " 'bowness',\n",
       " 'theyummylife',\n",
       " 'filled',\n",
       " 'gbejniet',\n",
       " 'posteing',\n",
       " 'shreveport',\n",
       " 'evens',\n",
       " 'partys',\n",
       " 'maintained',\n",
       " 'grisanti',\n",
       " 'lawlers',\n",
       " 'proceeds',\n",
       " 'stomach',\n",
       " 'loc',\n",
       " 'writers',\n",
       " 'believer',\n",
       " 'viola',\n",
       " 'crostini',\n",
       " '581mg',\n",
       " 'correct',\n",
       " 'granddad',\n",
       " 'operates',\n",
       " 'span',\n",
       " 'threads',\n",
       " 'raving',\n",
       " 'regulate',\n",
       " '107646',\n",
       " 'romantic',\n",
       " 'overnight',\n",
       " 'founded',\n",
       " 'varity',\n",
       " 'nanners',\n",
       " 'invite',\n",
       " 'southbeachdiet',\n",
       " 'bellagio',\n",
       " 'conditioned',\n",
       " 'uhhhh',\n",
       " 'mode',\n",
       " 'gisella',\n",
       " '2001',\n",
       " 'merits',\n",
       " 'elook',\n",
       " 'merrit',\n",
       " 'white',\n",
       " '167664',\n",
       " 'varicose',\n",
       " 'preserving',\n",
       " 'goody',\n",
       " 'bumper',\n",
       " 'blanched',\n",
       " 'eek',\n",
       " 'wala',\n",
       " 'cajuns',\n",
       " '31xo2m',\n",
       " 'pleasure',\n",
       " 'stemmed',\n",
       " 'arabian',\n",
       " 'dal',\n",
       " 'philippine',\n",
       " 'spinach',\n",
       " 'reeses',\n",
       " 'recpie',\n",
       " 'sos',\n",
       " 'credit',\n",
       " 'cavities',\n",
       " 'granmother',\n",
       " 'born',\n",
       " 'accomplished',\n",
       " 'long',\n",
       " 'indiana',\n",
       " 'antsy',\n",
       " 'zaars',\n",
       " '28g',\n",
       " 'nectar',\n",
       " 'bones',\n",
       " 'michele',\n",
       " 'potpie',\n",
       " 'jonafree',\n",
       " 'schezwan',\n",
       " 'pectin',\n",
       " 'recieve',\n",
       " 'deprive',\n",
       " 'reheat',\n",
       " 'ignored',\n",
       " 'soldier',\n",
       " 'horizons',\n",
       " 'foggy',\n",
       " '65',\n",
       " 'crimping',\n",
       " 'meal',\n",
       " 'mary',\n",
       " 'molding',\n",
       " 'trapped',\n",
       " 'cooled',\n",
       " 'murgh',\n",
       " 'deranged',\n",
       " 'borrower',\n",
       " 'err',\n",
       " 'mccausland',\n",
       " 'encrusted',\n",
       " 'hospital',\n",
       " '1990',\n",
       " 'mccartneys',\n",
       " 'neutralizes',\n",
       " 'dominant',\n",
       " 'tasing',\n",
       " 'romanoff',\n",
       " 'referred',\n",
       " 'depth',\n",
       " 'blackend',\n",
       " 'soiled',\n",
       " 'purpleturtle',\n",
       " 'encyclopedia',\n",
       " 'kolla',\n",
       " 'servers',\n",
       " 'julie',\n",
       " 'lable',\n",
       " 'pleasing',\n",
       " 'msir',\n",
       " 'moral',\n",
       " 'pretzles',\n",
       " '1973',\n",
       " 'tast',\n",
       " 'mmmmmmmmm',\n",
       " 'whitby',\n",
       " 'suitably',\n",
       " 'bento',\n",
       " 'vermont',\n",
       " 'cassserole',\n",
       " 'straits',\n",
       " 'pricy',\n",
       " 'hip2thrift',\n",
       " '41g',\n",
       " '46g',\n",
       " 'cia',\n",
       " 'islandteashop',\n",
       " 'unless',\n",
       " 'nelda',\n",
       " 'drummettes',\n",
       " 'dampers',\n",
       " 'shroom',\n",
       " 'stoup',\n",
       " 'sunday',\n",
       " 'boots',\n",
       " 'potine',\n",
       " 'zigato',\n",
       " 'cloyingly',\n",
       " 'ignore',\n",
       " 'dryed',\n",
       " 'partner',\n",
       " 'starches',\n",
       " 'hmmph',\n",
       " 'shells',\n",
       " 'davis',\n",
       " '26987',\n",
       " 'inspecting',\n",
       " 'grove',\n",
       " '201650',\n",
       " 'sunrise',\n",
       " 'corsica',\n",
       " 'greatest',\n",
       " 'designing',\n",
       " 'leftovers',\n",
       " 'nephews',\n",
       " 'whittle',\n",
       " 'basted',\n",
       " 'himalayan',\n",
       " 'halved',\n",
       " 'illinois',\n",
       " 'doctors',\n",
       " 'skinned',\n",
       " 'honour',\n",
       " 'thanking',\n",
       " 'villas',\n",
       " 'sum',\n",
       " 'bein',\n",
       " 'reputations',\n",
       " 'diabetic',\n",
       " 'hooks',\n",
       " 'anglaise',\n",
       " 'searchs',\n",
       " '155g',\n",
       " 'chocolately',\n",
       " 'wore',\n",
       " 'shelf',\n",
       " 'average',\n",
       " 'rodgers',\n",
       " 'send',\n",
       " 'pearl',\n",
       " 'reinforcement',\n",
       " 'janice',\n",
       " 'yucca',\n",
       " 'redhot',\n",
       " 'ibs',\n",
       " 'coentro',\n",
       " 'specified',\n",
       " 'flay',\n",
       " 'plainsboro',\n",
       " '1958',\n",
       " 'lish',\n",
       " 'surburbs',\n",
       " 'marked',\n",
       " 'molinillo',\n",
       " 'situation',\n",
       " 'magnet',\n",
       " 'hawaii',\n",
       " 'syllabub',\n",
       " 'published',\n",
       " 'english',\n",
       " 'festivus',\n",
       " 'deepen',\n",
       " 'govind',\n",
       " 'belacan',\n",
       " 'brats',\n",
       " '30',\n",
       " 'confectioners',\n",
       " 'lumps',\n",
       " 'pikliz',\n",
       " '7bwgul',\n",
       " 'fyi',\n",
       " 'parmerona',\n",
       " 'fizzano',\n",
       " 'liz',\n",
       " 'okstate',\n",
       " '9272',\n",
       " 'tom921',\n",
       " 'columbian',\n",
       " 'putzing',\n",
       " 'zippy',\n",
       " 'gylcemic',\n",
       " 'kreiger',\n",
       " '3g',\n",
       " 'winco',\n",
       " 'model',\n",
       " 'um',\n",
       " 'bier',\n",
       " 'scuse',\n",
       " '66',\n",
       " 'pasticciata',\n",
       " 'essaouira',\n",
       " 'gnudi',\n",
       " 'supermarket',\n",
       " 'multitasking',\n",
       " 'andrea',\n",
       " 'accuses',\n",
       " 'necesary',\n",
       " 'gulf',\n",
       " 'doe',\n",
       " 'teeny',\n",
       " 'varition',\n",
       " 'hog',\n",
       " 'thidabeau',\n",
       " 'kewra',\n",
       " 'bites',\n",
       " 'baigan',\n",
       " 'trimmed',\n",
       " 'carts',\n",
       " 'hefty',\n",
       " 'stocking',\n",
       " 'legendary',\n",
       " 'tamarac',\n",
       " 'seats',\n",
       " 'unstuff',\n",
       " 'circles',\n",
       " 'pike',\n",
       " 'nobody',\n",
       " 'vehicle',\n",
       " 'peach',\n",
       " 'radichio',\n",
       " 'embellish',\n",
       " 'rehydrate',\n",
       " 'riffraff',\n",
       " 'lowfat',\n",
       " 'suace',\n",
       " '412',\n",
       " 'jug',\n",
       " 'addict',\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Загрузим файл preprocessed_descriptions.csv\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Объединим все описания в один текст\n",
    "all_descriptions = ' '.join(data['preprocessed_descriptions'].astype(str))\n",
    "\n",
    "# Токенизация текста\n",
    "tokens = word_tokenize(all_descriptions)\n",
    "\n",
    "# Преобразование в набор уникальных слов\n",
    "unique_words = set(tokens)\n",
    "\n",
    "# Вывод уникальных слов\n",
    "unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пар -> между: 5\n",
      "Сгенерируйте -> пар: 11\n",
      "редактирования -> посчитайте: 12\n",
      "и -> выбранных: 9\n",
      "расстояние -> ними: 9\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import Levenshtein\n",
    "\n",
    "# Сгенерировать 5 пар случайных слов\n",
    "pairs = [(random.choice(words), random.choice(words)) for i in range(5)]\n",
    "\n",
    "# Посчитать расстояние редактирования для каждой пары\n",
    "distances = [Levenshtein.distance(pair[0], pair[1]) for pair in pairs]\n",
    "\n",
    "# Вывести пары слов и расстояния редактирования\n",
    "for pair, distance in zip(pairs, distances):\n",
    "    print(f\"{pair[0]} -> {pair[1]}: {distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['солнце',\n",
       " 'солнц',\n",
       " 'солне',\n",
       " 'сонце',\n",
       " 'солнца',\n",
       " 'солнцу',\n",
       " 'солнцы',\n",
       " 'солнцем',\n",
       " 'соне',\n",
       " 'солн']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def find_k_nearest_neighbors(word, words, k):\n",
    "    # Посчитать расстояния Левенштейна между заданным словом и всеми словами из списка\n",
    "    distances = [Levenshtein.distance(word, w) for w in words]\n",
    "\n",
    "    # Отсортировать слова по расстоянию Левенштейна\n",
    "    sorted_words = sorted(zip(distances, words), key=lambda x: x[0])\n",
    "\n",
    "    # Вернуть k ближайших соседей\n",
    "    return [w for d, w in sorted_words[:k]]\n",
    "\n",
    "find_k_nearest_neighbors('солнце', words, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shubs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shubs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shubs\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bahamian</th>\n",
       "      <td>bahamian</td>\n",
       "      <td>bahamian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>struck</th>\n",
       "      <td>struck</td>\n",
       "      <td>struck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordial</th>\n",
       "      <td>cordial</td>\n",
       "      <td>cordial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>available</th>\n",
       "      <td>avail</td>\n",
       "      <td>available</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streamline</th>\n",
       "      <td>streamlin</td>\n",
       "      <td>streamline</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           stemmed_word normalized_word\n",
       "word                                   \n",
       "bahamian       bahamian        bahamian\n",
       "struck           struck          struck\n",
       "cordial         cordial         cordial\n",
       "available         avail       available\n",
       "streamline    streamlin      streamline"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Загрузим необходимые компоненты NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "df = pd.DataFrame(list(unique_words), columns=['word'])\n",
    "\n",
    "# Инициализация SnowballStemmer и WordNetLemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Функции для стемминга и лемматизации\n",
    "df['stemmed_word'] = df['word'].apply(lambda x: stemmer.stem(x))\n",
    "df['normalized_word'] = df['word'].apply(lambda x: lemmatizer.lemmatize(x))\n",
    "\n",
    "# Указываем столбец word в качестве индекса\n",
    "df.set_index('word', inplace=True)\n",
    "\n",
    "# Вывод первых строк DataFrame для проверки\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shubs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4709755748661407,\n",
       " [('the', 40413),\n",
       "  ('a', 35131),\n",
       "  ('and', 30585),\n",
       "  ('i', 27945),\n",
       "  ('this', 27181),\n",
       "  ('to', 23598),\n",
       "  ('it', 23300),\n",
       "  ('is', 20306),\n",
       "  ('of', 18405),\n",
       "  ('for', 16023)],\n",
       " [('recipe', 15198),\n",
       "  ('make', 6438),\n",
       "  ('time', 5287),\n",
       "  ('use', 4652),\n",
       "  ('great', 4522),\n",
       "  ('like', 4276),\n",
       "  ('easy', 4263),\n",
       "  ('one', 4018),\n",
       "  ('good', 3887),\n",
       "  ('made', 3874)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Объединим все описания в один текст\n",
    "all_descriptions = ' '.join(data['preprocessed_descriptions'].astype(str))\n",
    "\n",
    "# Токенизация текста\n",
    "tokens = word_tokenize(all_descriptions)\n",
    "\n",
    "# Получим список стоп-слов для английского языка\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Подсчет частоты слов до удаления стоп-слов\n",
    "word_freq_before = Counter(tokens)\n",
    "\n",
    "# Удаление стоп-слов\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Подсчет частоты слов после удаления стоп-слов\n",
    "word_freq_after = Counter(filtered_tokens)\n",
    "\n",
    "# Доля стоп-слов от общего количества слов\n",
    "total_words = len(tokens)\n",
    "stop_words_count = total_words - len(filtered_tokens)\n",
    "stop_words_ratio = stop_words_count / total_words\n",
    "\n",
    "# Топ-10 самых часто употребляемых слов до и после удаления стоп-слов\n",
    "top_10_before = word_freq_before.most_common(10)\n",
    "top_10_after = word_freq_after.most_common(10)\n",
    "\n",
    "stop_words_ratio, top_10_before, top_10_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторное представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Выберите случайным образом 5 рецептов из набора данных. Представьте описание каждого рецепта в виде числового вектора при помощи `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>preprocessed_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>barbecued potatoes</td>\n",
       "      <td>this is a delicious side that s great for pot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>reena s pickled beets</td>\n",
       "      <td>these are my mom s beets   she taught me to ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>saucy mocha pots of cream  microwave easy fix</td>\n",
       "      <td>w ea pg i have turned  i have fallen more in l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>spaghetti salad</td>\n",
       "      <td>great salad  no bottled dressings or odd ingre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>beef steaks with capsicum relish</td>\n",
       "      <td>this is a really easy way to prepare steak for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "2308                              barbecued potatoes   \n",
       "22404                          reena s pickled beets   \n",
       "23397  saucy mocha pots of cream  microwave easy fix   \n",
       "25058                                spaghetti salad   \n",
       "2664                beef steaks with capsicum relish   \n",
       "\n",
       "                               preprocessed_descriptions  \n",
       "2308   this is a delicious side that s great for pot ...  \n",
       "22404  these are my mom s beets   she taught me to ma...  \n",
       "23397  w ea pg i have turned  i have fallen more in l...  \n",
       "25058  great salad  no bottled dressings or odd ingre...  \n",
       "2664   this is a really easy way to prepare steak for...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ago</th>\n",
       "      <th>ahead</th>\n",
       "      <th>also</th>\n",
       "      <th>amounts</th>\n",
       "      <th>and</th>\n",
       "      <th>another</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>australian</th>\n",
       "      <th>...</th>\n",
       "      <th>weekly</th>\n",
       "      <th>what</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>womens</th>\n",
       "      <th>work</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2308</th>\n",
       "      <td>0.111007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155031</td>\n",
       "      <td>0.111007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.13759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111007</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>0.097391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120713</td>\n",
       "      <td>0.120713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120713</td>\n",
       "      <td>0.097391</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.120713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.149673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.185516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.185516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25058</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107195</td>\n",
       "      <td>0.107195</td>\n",
       "      <td>0.107195</td>\n",
       "      <td>0.060392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107195</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.321586</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071790</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 154 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ago     ahead      also   amounts       and   another       any  \\\n",
       "2308   0.111007  0.000000  0.000000  0.000000  0.155031  0.111007  0.000000   \n",
       "22404  0.097391  0.000000  0.000000  0.000000  0.204023  0.000000  0.000000   \n",
       "23397  0.000000  0.000000  0.000000  0.000000  0.000000  0.149673  0.000000   \n",
       "25058  0.000000  0.000000  0.000000  0.000000  0.187091  0.000000  0.000000   \n",
       "2664   0.000000  0.107195  0.107195  0.107195  0.060392  0.000000  0.107195   \n",
       "\n",
       "            are        as  australian  ...    weekly      what      who  \\\n",
       "2308   0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.13759   \n",
       "22404  0.120713  0.120713    0.000000  ...  0.000000  0.000000  0.00000   \n",
       "23397  0.000000  0.000000    0.185516  ...  0.185516  0.000000  0.00000   \n",
       "25058  0.000000  0.000000    0.000000  ...  0.000000  0.000000  0.00000   \n",
       "2664   0.000000  0.000000    0.000000  ...  0.000000  0.107195  0.00000   \n",
       "\n",
       "           will      with    womens      work     years       you     young  \n",
       "2308   0.000000  0.000000  0.000000  0.000000  0.111007  0.000000  0.000000  \n",
       "22404  0.000000  0.000000  0.000000  0.120713  0.097391  0.161686  0.120713  \n",
       "23397  0.185516  0.000000  0.185516  0.000000  0.000000  0.124242  0.000000  \n",
       "25058  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2664   0.000000  0.321586  0.000000  0.000000  0.000000  0.071790  0.000000  \n",
       "\n",
       "[5 rows x 154 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Загрузка данных\n",
    "file_path = 'preprocessed_descriptions.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Выбор случайных 5 рецептов\n",
    "random_recipes = data.sample(n=5, random_state=42)\n",
    "\n",
    "# Получение описаний выбранных рецептов\n",
    "descriptions = random_recipes['preprocessed_descriptions'].astype(str).tolist()\n",
    "\n",
    "# Инициализация TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Применение TfidfVectorizer к описаниям рецептов\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "# Преобразование результата в DataFrame для удобства\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=random_recipes.index, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Вывод результатов\n",
    "display(random_recipes)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Вычислите близость между каждой парой рецептов, выбранных в задании 3.1, используя косинусное расстояние (`scipy.spatial.distance.cosine`) Результаты оформите в виде таблицы `pd.DataFrame`. В качестве названий строк и столбцов используйте названия рецептов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>barbecued potatoes</th>\n",
       "      <th>reena s pickled beets</th>\n",
       "      <th>saucy mocha pots of cream  microwave easy fix</th>\n",
       "      <th>spaghetti salad</th>\n",
       "      <th>beef steaks with capsicum relish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>barbecued potatoes</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.694352</td>\n",
       "      <td>0.915225</td>\n",
       "      <td>0.891019</td>\n",
       "      <td>0.809012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reena s pickled beets</th>\n",
       "      <td>0.694352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923048</td>\n",
       "      <td>0.961829</td>\n",
       "      <td>0.818986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saucy mocha pots of cream  microwave easy fix</th>\n",
       "      <td>0.915225</td>\n",
       "      <td>0.923048</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.886841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spaghetti salad</th>\n",
       "      <td>0.891019</td>\n",
       "      <td>0.961829</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.972735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beef steaks with capsicum relish</th>\n",
       "      <td>0.809012</td>\n",
       "      <td>0.818986</td>\n",
       "      <td>0.886841</td>\n",
       "      <td>0.972735</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               barbecued potatoes  \\\n",
       "barbecued potatoes                                       0.000000   \n",
       "reena s pickled beets                                    0.694352   \n",
       "saucy mocha pots of cream  microwave easy fix            0.915225   \n",
       "spaghetti salad                                          0.891019   \n",
       "beef steaks with capsicum relish                         0.809012   \n",
       "\n",
       "                                               reena s pickled beets  \\\n",
       "barbecued potatoes                                          0.694352   \n",
       "reena s pickled beets                                       0.000000   \n",
       "saucy mocha pots of cream  microwave easy fix               0.923048   \n",
       "spaghetti salad                                             0.961829   \n",
       "beef steaks with capsicum relish                            0.818986   \n",
       "\n",
       "                                               saucy mocha pots of cream  microwave easy fix  \\\n",
       "barbecued potatoes                                                                  0.915225   \n",
       "reena s pickled beets                                                               0.923048   \n",
       "saucy mocha pots of cream  microwave easy fix                                       0.000000   \n",
       "spaghetti salad                                                                     1.000000   \n",
       "beef steaks with capsicum relish                                                    0.886841   \n",
       "\n",
       "                                               spaghetti salad  \\\n",
       "barbecued potatoes                                    0.891019   \n",
       "reena s pickled beets                                 0.961829   \n",
       "saucy mocha pots of cream  microwave easy fix         1.000000   \n",
       "spaghetti salad                                       0.000000   \n",
       "beef steaks with capsicum relish                      0.972735   \n",
       "\n",
       "                                               beef steaks with capsicum relish  \n",
       "barbecued potatoes                                                     0.809012  \n",
       "reena s pickled beets                                                  0.818986  \n",
       "saucy mocha pots of cream  microwave easy fix                          0.886841  \n",
       "spaghetti salad                                                        0.972735  \n",
       "beef steaks with capsicum relish                                       0.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "titles = random_recipes['name'].tolist()\n",
    "\n",
    "# Применим TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions).toarray()\n",
    "\n",
    "# Вычисление косинусного расстояния между каждой парой рецептов\n",
    "distances = pd.DataFrame(index=titles, columns=titles)\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    for j in range(len(titles)):\n",
    "        if i == j:\n",
    "            distances.iloc[i, j] = 0.0\n",
    "        else:\n",
    "            distances.iloc[i, j] = cosine(tfidf_matrix[i], tfidf_matrix[j])\n",
    "\n",
    "# Преобразование результатов в числовой формат\n",
    "distances = distances.astype(float)\n",
    "\n",
    "# Вывод таблицы с косинусными расстояниями\n",
    "distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Какие рецепты являются наиболее похожими? Прокомментируйте результат (словами)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее похожие - те, которые ближе к 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
